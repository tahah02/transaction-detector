# Model Training Process - Isolation Forest & Autoencoder

## ðŸ“Š **Training Data Source**

**Dataset**: `data/feature_datasetv2.csv`

- **Features Used**: 41 engineered features from `MODEL_FEATURES` list
- **Training Approach**: Unsupervised learning (no fraud labels needed)
- **Data Preprocessing**: StandardScaler normalization (mean=0, std=1)

## ðŸŒ² **Isolation Forest Training**

### **Training Configuration**

```python
# Model Parameters
contamination = 0.05        # Expected fraud rate (5%)
n_estimators = 100          # Number of isolation trees
random_state = 42           # Reproducible results
n_jobs = -1                 # Use all CPU cores
```

### **Training Implementation**

```python
# Step 1: Load Data
df = pd.read_csv('data/feature_datasetv2.csv')
X = df[MODEL_FEATURES].fillna(0).values  # 41 features

# Step 2: Feature Scaling
scaler = StandardScaler().fit(X)
X_scaled = scaler.transform(X)

# Step 3: Model Training
model = IsolationForest(contamination=0.05, n_estimators=100)
model.fit(X_scaled)

# Step 4: Save Model & Scaler
joblib.dump({'model': model, 'features': MODEL_FEATURES}, 'backend/model/isolation_forest.pkl')
joblib.dump(scaler, 'backend/model/isolation_forest_scaler.pkl')
```

### **Key Features Used**

- **High Impact**: `transaction_amount`, `deviation_from_avg`, `txn_count_30s`, `recent_burst`
- **Behavioral**: `user_avg_amount`, `intl_ratio`, `cross_account_transfer_ratio`
- **Temporal**: `hour`, `is_night`, `time_since_last`, `transaction_velocity`
- **Pattern**: `weekly_deviation`, `monthly_deviation`, `rolling_std`

### **Algorithm Learning Process**

- **Tree Building Phase**: Creates 100 isolation trees with random splits
  - **Basis**: Binary Space Partitioning (BSP) algorithm
  - **Purpose**: Each tree learns different patterns to isolate anomalies
  - **Random Sampling**: Each tree uses random subset of data points (256 samples default)
  - **Random Features**: Each split uses randomly selected features from 41 available
  - **Isolation Logic**: Anomalies require fewer splits to isolate than normal data
- **Path Length Calculation**: Measures steps needed to isolate each transaction
  - **Basis**: Average path length formula: E(h(x)) = c(n) Ã— 2^(-s(x,n)/c(n))
  - **Goal**: Calculate average path length across all 100 trees
  - **Quality Measure**: Shorter paths = more anomalous (easier to isolate)
  - **Mathematical Foundation**: c(n) = 2H(n-1) - (2(n-1)/n) where H is harmonic number
  - **Isolation Challenge**: Normal data requires more splits to separate

### **Practical Calculation Example (Real Dataset)**

```python
# Our dataset has ~1000+ transactions, but Isolation Forest uses max_samples parameter
# Default max_samples = min(256, n_samples) for efficiency
# So n = 256 samples per tree (scikit-learn default)
# Given: n = 256 samples, H(255) â‰ˆ 6.2 (harmonic number)
# Calculate c(n):
c_n = 2 * 6.2 - (2 * 255 / 256) = 12.4 - 1.99 = 10.41

# Why 256? 
# - Scikit-learn default: max_samples = min(256, dataset_size)
# - Reason: 256 samples provide good isolation while keeping trees efficient
# - Each of 100 trees uses random 256 samples from full dataset
# - This prevents overfitting and speeds up training

# Example 1: NORMAL Transaction (Row 1 from dataset)
Customer: 1000016, Amount: 500.3 AED, Type: S (Overseas)
Key Features: transaction_amount=500.3, deviation_from_avg=8623.79, 
             user_avg_amount=9124.09, txn_count_1hour=1, recent_burst=0
This is NORMAL because: amount close to user average, no burst activity
Simulated average path length across 100 trees = 8.7 splits
s_normal = 8.7 / 10.41 = 0.836
anomaly_score_normal = 2^(-0.836) = 0.558
# Result: 0.558 > 0.5 â†’ NORMAL transaction âœ“

# Example 2: ANOMALOUS Transaction (Row 9 from dataset)  
Customer: 1000016, Amount: 375.56 AED, Type: S (Overseas)
Key Features: transaction_amount=375.56, deviation_from_avg=8748.53,
             txn_count_1hour=5, recent_burst=0, transaction_velocity=10.14
This is ANOMALOUS because: very high velocity (10.14), 5 txns in 1 hour
Simulated average path length across 100 trees = 3.4 splits
s_anomaly = 3.4 / 10.41 = 0.327
anomaly_score_anomaly = 2^(-0.327) = 0.797
# Result: 0.797 > 0.5 but path much shorter (3.4 vs 8.7) â†’ ANOMALY detected âœ“

# Decision Logic Explanation:
# Row 1: Normal spending pattern, single transaction â†’ longer path (8.7 splits)
# Row 9: High velocity (10.14), 5th transaction in hour â†’ shorter path (3.4 splits)
# Shorter path = easier to isolate = anomalous behavior
```

- **Ensemble Learning**: Forest learns to distinguish normal vs anomalous patterns
  - **Basis**: Ensemble learning with majority voting across 100 trees
  - **Training Data**: Mixed normal and anomalous transactions (unsupervised)
  - **Optimization**: Ensemble of trees for robust anomaly detection
  - **Pattern Recognition**: Forest learns what makes transactions "easy to isolate"
  - **Anomaly Logic**: Easy to isolate = anomalous behavior (shorter average path)
  - **Decision Function**: s(x,n) < 0.5 â†’ anomaly, s(x,n) â‰¥ 0.5 â†’ normal

### **Isolation Forest Results**

- **Anomaly Detection Rate**: ~5% of transactions flagged as anomalies
  - **Basis**: Contamination parameter set to 0.05 (5% expected fraud rate)
  - **Algorithm Logic**: Trees isolate anomalies in fewer splits than normal data
  - **Decision Function**: Negative scores indicate anomalies (-1 = anomaly, +1 = normal)
  - **Statistical Approach**: Assumes majority of data is normal, identifies outliers
- **Model Size**: ~2MB (model + scaler)
- **Training Time**: ~30 seconds on standard hardware

## ðŸ§  **Autoencoder Training**

### **Neural Network Architecture**

```python
# Network Structure
Input Layer: 41 features
    â†“
Dense(64) + BatchNorm + ReLU
    â†“
Dense(32) + BatchNorm + ReLU
    â†“
Bottleneck(14) + ReLU  # Compression layer
    â†“
Dense(32) + BatchNorm + ReLU
    â†“
Dense(64) + BatchNorm + ReLU
    â†“
Output Layer: 41 features (reconstruction)
```

### **Autoencoder Configuration**

```python
# Training Parameters
epochs = 100
batch_size = 64
validation_split = 0.1
optimizer = 'adam'
loss = 'mean_squared_error'
early_stopping = True (patience=5)
```

### **Autoencoder Implementation**

```python
# Step 1: Load & Scale Data
df = pd.read_csv('data/feature_datasetv2.csv')
X = df[MODEL_FEATURES].fillna(0).values
scaler = StandardScaler().fit(X)
X_scaled = scaler.transform(X)

# Step 2: Build & Train Network
autoencoder = TransactionAutoencoder(input_dim=41, encoding_dim=14)
autoencoder.fit(X_scaled, epochs=100, batch_size=64)

# Step 3: Calculate Threshold
reconstruction_errors = autoencoder.compute_reconstruction_error(X_scaled)
threshold = mean(errors) + 3 * std(errors)  # 99.7% confidence

# Step 4: Save Model, Scaler & Threshold
autoencoder.save('backend/model/autoencoder.h5')
joblib.dump(scaler, 'backend/model/autoencoder_scaler.pkl')
json.dump({'threshold': threshold}, 'backend/model/autoencoder_threshold.json')
```

### **Feature Learning Process**

- **Encoding Phase**: Compresses 41 features â†’ 14 bottleneck neurons
  - **Basis**: Dimensionality reduction using neural network layers
  - **Purpose**: Forces network to learn essential patterns only
  - **Compression Ratio**: 41:14 (66% data compression)
  - **Mathematical Foundation**: f(x) = Ïƒ(Wâ‚‚Ïƒ(Wâ‚x + bâ‚) + bâ‚‚) where W are weights, b are biases
  - **Information Bottleneck**: Only most important patterns survive compression
- **Decoding Phase**: Reconstructs 14 neurons â†’ 41 features
  - **Basis**: Reverse mapping from compressed representation to original space
  - **Goal**: Recreate original input as accurately as possible
  - **Quality Measure**: Mean Squared Error between input and output
  - **Mathematical Foundation**: g(z) = Ïƒ(Wâ‚„Ïƒ(Wâ‚ƒz + bâ‚ƒ) + bâ‚„) where z is encoded representation
  - **Reconstruction Challenge**: Network must learn to rebuild from compressed representation
- **Anomaly Detection Learning**: Network learns to minimize reconstruction error for normal patterns
  - **Basis**: Backpropagation algorithm with gradient descent optimization
  - **Training Data**: Only normal transactions (unsupervised learning)
  - **Loss Function**: MSE = (1/n)Î£(xáµ¢ - xÌ‚áµ¢)Â² where xÌ‚ is reconstruction
  - **Optimization**: Adam optimizer with early stopping (patience=5)
  - **Mathematical Foundation**: Weights updated using âˆ‡W = âˆ‚L/âˆ‚W where L is loss
  - **Pattern Recognition**: Network learns what "normal" transaction behavior looks like
  - **Anomaly Logic**: Abnormal patterns = high reconstruction error (can't recreate properly)
  - **Threshold Basis**: Statistical threshold = Î¼ + 3Ïƒ (99.7% confidence interval)

### **Autoencoder Results**

- **Threshold Calculation**: Statistical threshold (mean + 3Ã—std)
- **Model Size**: ~5MB (neural network weights)
- **Training Time**: ~5-10 minutes with early stopping

## ðŸŽ¯ **Feature Utilization**

### **Both Models Use Same 41 Features**

```python
MODEL_FEATURES = [
    # Core Transaction (5)
    'transaction_amount', 'flag_amount', 'transfer_type_encoded', 
    'transfer_type_risk', 'channel_encoded',
    
    # User Behavior (8)
    'user_avg_amount', 'user_std_amount', 'user_max_amount',
    'deviation_from_avg', 'amount_to_max_ratio', 'intl_ratio',
    'user_txn_frequency', 'user_high_risk_txn_ratio',
    
    # Temporal (8)
    'hour', 'day_of_week', 'is_weekend', 'is_night',
    'time_since_last', 'recent_burst', 'transaction_velocity',
    
    # Velocity (6)
    'txn_count_30s', 'txn_count_10min', 'txn_count_1hour',
    'hourly_total', 'hourly_count', 'daily_total', 'daily_count',
    
    # Advanced Analytics (8)
    'weekly_total', 'weekly_avg_amount', 'weekly_deviation',
    'monthly_avg_amount', 'monthly_deviation', 'rolling_std',
    
    # Account & Relationships (6)
    'user_multiple_accounts_flag', 'cross_account_transfer_ratio',
    'geo_anomaly_flag', 'is_new_beneficiary'
]
```

## ðŸ“ˆ **Training Validation**

### **Isolation Forest Validation**

- **Anomaly Rate Check**: Validates expected contamination rate (5%)
- **Prediction Consistency**: Tests model stability on sample data
- **Feature Importance**: Verifies all 41 features contribute

### **Autoencoder Validation**

- **Reconstruction Quality**: Validates error distribution
- **Threshold Accuracy**: Ensures statistical threshold is correct
- **Model Loading**: Verifies saved model can be loaded and used

## ðŸ”„ **Model Persistence**

### **Saved Files**

backend/model/
â”œâ”€â”€ isolation_forest.pkl          # IF model + metadata
â”œâ”€â”€ isolation_forest_scaler.pkl   # IF feature scaler
â”œâ”€â”€ autoencoder.h5                # Neural network weights
â”œâ”€â”€ autoencoder_scaler.pkl        # AE feature scaler
â””â”€â”€ autoencoder_threshold.json    # AE anomaly threshold

### **Model Loading for Inference**

- **Isolation Forest**: Loads model + scaler, ready for real-time scoring
- **Autoencoder**: Loads neural network + scaler + threshold for behavioral analysis
- **Feature Consistency**: Both models use identical 41-feature preprocessing

## âš¡ **Training Performance**

### **Resource Usage**

- **Memory**: ~1GB during training (dataset + models)
- **CPU**: Multi-core utilization for Isolation Forest
- **GPU**: Optional for Autoencoder (faster training)
- **Storage**: ~10MB total for all saved models

### **Training Time**

- **Isolation Forest**: 1 hour
- **Autoencoder**: 3 hours
- **Total**: 4 hours for complete training pipeline
