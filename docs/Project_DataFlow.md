# Banking Fraud Detection System - Complete Data Flow

## ğŸŒŠ **System Data Flow Overview**

The Banking Fraud Detection System processes transaction data through multiple stages, from raw input to final fraud decision. This document details every step of data transformation and processing.

## ğŸ“Š **High-Level Data Flow Architecture**

```
Raw Transaction â†’ Feature Engineering â†’ Triple Detection â†’ Final Decision
       â†“                    â†“                â†“              â†“
   CSV Input         41 Features      Rule + ML + AE    Block/Approve
```

## ğŸ”„ **Detailed Data Flow Stages**

### **Stage 1: Data Input & Validation**

#### **Input Sources**
```
ğŸ“¥ Transaction Input
â”œâ”€â”€ ğŸŒ Streamlit Web Interface
â”‚   â”œâ”€â”€ User authentication
â”‚   â”œâ”€â”€ Account selection
â”‚   â”œâ”€â”€ Transaction form input
â”‚   â””â”€â”€ Real-time validation
â”œâ”€â”€ ğŸ“Š CSV File Upload
â”‚   â”œâ”€â”€ Bulk transaction processing
â”‚   â”œâ”€â”€ Historical data analysis
â”‚   â””â”€â”€ Batch fraud detection
â””â”€â”€ ğŸ”Œ API Integration (Future)
    â”œâ”€â”€ Real-time banking systems
    â”œâ”€â”€ Mobile app transactions
    â””â”€â”€ ATM transaction feeds
```

#### **Raw Transaction Fields**
```python
# Core Transaction Data
{
    'CustomerId': '1000016',
    'TransferType': 'S',  # S=Overseas, I=Ajman, L=UAE, Q=Quick, O=Own
    'FromAccountNo': '11000016019',
    'ReceipentAccount': 'DE89370400440532013000',
    'Amount': 119,
    'Currency': 'EUR',
    'CreateDate': '07/05/2025 16:17',
    'ChannelId': 1,
    'BankCountry': 'Germany',
    # ... additional fields
}
```

### **Stage 2: Feature Engineering Pipeline**

#### **Data Preprocessing**
```
ğŸ“Š Raw Data Processing
â”œâ”€â”€ ğŸ• DateTime Parsing
â”‚   â”œâ”€â”€ CreateDate â†’ pandas datetime
â”‚   â”œâ”€â”€ Extract hour, day_of_week
â”‚   â”œâ”€â”€ Calculate is_weekend, is_night
â”‚   â””â”€â”€ Handle timezone conversions
â”œâ”€â”€ ğŸ’° Amount Normalization
â”‚   â”œâ”€â”€ Convert to AED base currency
â”‚   â”œâ”€â”€ Handle currency conversions
â”‚   â”œâ”€â”€ Validate amount ranges
â”‚   â””â”€â”€ Clean invalid values
â””â”€â”€ ğŸ·ï¸ Categorical Encoding
    â”œâ”€â”€ TransferType â†’ encoded values
    â”œâ”€â”€ ChannelId â†’ channel_encoded
    â”œâ”€â”€ BankCountry â†’ geo features
    â””â”€â”€ Risk score mappings
```

#### **41 Feature Generation Process**

##### **Basic Transaction Features (5 features)**
```python
# Direct transaction attributes
transaction_amount = AmountInAed
flag_amount = 1 if TransferType == 'S' else 0  # International flag
transfer_type_encoded = TRANSFER_TYPE_ENCODED[TransferType]
transfer_type_risk = TRANSFER_TYPE_RISK[TransferType]
channel_encoded = channel_mapping[ChannelId]
```

##### **Temporal Features (8 features)**
```python
# Time-based patterns
hour = CreateDate.hour  # 0-23
day_of_week = CreateDate.dayofweek  # 0-6
is_weekend = 1 if day_of_week >= 5 else 0
is_night = 1 if hour < 6 or hour >= 22 else 0
time_since_last = current_time - last_transaction_time
recent_burst = 1 if time_since_last < 300 else 0  # 5 minutes
transaction_velocity = 1 / (time_since_last / 3600)  # per hour
```

##### **User Behavioral Features (8 features)**
```python
# Historical user patterns
user_avg_amount = mean(user_historical_amounts)
user_std_amount = std(user_historical_amounts)
user_max_amount = max(user_historical_amounts)
user_txn_frequency = count(user_transactions)
deviation_from_avg = abs(current_amount - user_avg_amount)
amount_to_max_ratio = current_amount / user_max_amount
intl_ratio = count(international_txns) / total_txns
user_high_risk_txn_ratio = count(high_risk_txns) / total_txns
```

##### **Account & Beneficiary Features (6 features)**
```python
# Account usage patterns
num_accounts = unique_count(user_accounts)
user_multiple_accounts_flag = 1 if num_accounts > 1 else 0
cross_account_transfer_ratio = cross_account_txns / total_txns
geo_anomaly_flag = 1 if unique_countries > 2 else 0
is_new_beneficiary = 1 if beneficiary not in history else 0
beneficiary_txn_count_30d = count(beneficiary_txns_last_30d)
```

##### **Velocity & Frequency Features (6 features)**
```python
# Transaction velocity tracking
txn_count_30s = count(transactions_last_30_seconds)
txn_count_10min = count(transactions_last_10_minutes)
txn_count_1hour = count(transactions_last_1_hour)
hourly_total = sum(amounts_this_hour)
hourly_count = count(transactions_this_hour)
daily_total = sum(amounts_today)
daily_count = count(transactions_today)
```

##### **Advanced Analytics Features (8 features)**
```python
# Weekly patterns
weekly_total = sum(amounts_this_week)
weekly_txn_count = count(transactions_this_week)
weekly_avg_amount = mean(amounts_this_week)
weekly_deviation = abs(current_amount - weekly_avg_amount)
amount_vs_weekly_avg = current_amount / weekly_avg_amount

# Monthly patterns
current_month_spending = sum(amounts_this_month)
monthly_txn_count = count(transactions_this_month)
monthly_avg_amount = mean(amounts_this_month)
monthly_deviation = abs(current_amount - monthly_avg_amount)
amount_vs_monthly_avg = current_amount / monthly_avg_amount

# Statistical measures
rolling_std = std(last_5_transactions)
```

### **Stage 3: Triple Detection Pipeline**

#### **Detection Layer 1: Rule Engine**
```
ğŸš« Business Rules Processing
â”œâ”€â”€ ğŸ“Š Input: 41 engineered features
â”œâ”€â”€ ğŸ” Velocity Checks
â”‚   â”œâ”€â”€ txn_count_30s â‰¤ 2
â”‚   â”œâ”€â”€ txn_count_10min â‰¤ 5
â”‚   â”œâ”€â”€ txn_count_1hour â‰¤ 15
â”‚   â””â”€â”€ recent_burst validation
â”œâ”€â”€ ğŸ’° Amount Limits
â”‚   â”œâ”€â”€ transaction_amount â‰¤ user_limit
â”‚   â”œâ”€â”€ daily_total â‰¤ daily_limit
â”‚   â”œâ”€â”€ weekly_total â‰¤ weekly_limit
â”‚   â””â”€â”€ monthly_total â‰¤ monthly_limit
â”œâ”€â”€ ğŸŒ Transfer Type Rules
â”‚   â”œâ”€â”€ Overseas (S): stricter limits
â”‚   â”œâ”€â”€ UAE (L): standard limits
â”‚   â”œâ”€â”€ Quick (Q): medium limits
â”‚   â””â”€â”€ Own (O): relaxed limits
â””â”€â”€ ğŸ“¤ Output: BLOCK/PASS + violation details
```

#### **Detection Layer 2: Isolation Forest**
```
ğŸŒ² Statistical Anomaly Detection
â”œâ”€â”€ ğŸ“Š Input: 41 normalized features
â”œâ”€â”€ ğŸ”„ Processing Pipeline
â”‚   â”œâ”€â”€ Feature validation (all 41 present)
â”‚   â”œâ”€â”€ StandardScaler normalization
â”‚   â”œâ”€â”€ Isolation Forest prediction
â”‚   â””â”€â”€ Anomaly score calculation
â”œâ”€â”€ ğŸ¯ Decision Logic
â”‚   â”œâ”€â”€ prediction = model.predict(features)
â”‚   â”œâ”€â”€ anomaly_score = model.decision_function(features)
â”‚   â”œâ”€â”€ is_anomaly = prediction == -1
â”‚   â””â”€â”€ confidence = abs(anomaly_score)
â””â”€â”€ ğŸ“¤ Output: anomaly_score, is_anomaly, confidence
```

#### **Detection Layer 3: Autoencoder**
```
ğŸ§  Behavioral Pattern Analysis
â”œâ”€â”€ ğŸ“Š Input: 41 normalized features
â”œâ”€â”€ ğŸ”„ Neural Network Processing
â”‚   â”œâ”€â”€ Feature validation and scaling
â”‚   â”œâ”€â”€ Forward pass through network
â”‚   â”œâ”€â”€ Reconstruction error calculation
â”‚   â””â”€â”€ Threshold comparison
â”œâ”€â”€ ğŸ—ï¸ Network Architecture
â”‚   â”œâ”€â”€ Input Layer: 41 features
â”‚   â”œâ”€â”€ Encoder: [64, 32] â†’ 14 (bottleneck)
â”‚   â”œâ”€â”€ Decoder: 14 â†’ [32, 64] â†’ 41
â”‚   â””â”€â”€ Reconstruction: MSE loss
â”œâ”€â”€ ğŸ¯ Anomaly Detection
â”‚   â”œâ”€â”€ reconstruction_error = MSE(input, output)
â”‚   â”œâ”€â”€ is_anomaly = error > threshold
â”‚   â”œâ”€â”€ threshold = mean + 3*std (from training)
â”‚   â””â”€â”€ confidence = error / threshold
â””â”€â”€ ğŸ“¤ Output: reconstruction_error, is_anomaly, confidence
```

### **Stage 4: Decision Aggregation**

#### **Hybrid Decision Logic**
```python
def make_final_decision(rule_result, if_result, ae_result):
    # Priority 1: Hard business rule violations
    if rule_result['blocked']:
        return {
            'decision': 'BLOCKED',
            'reason': rule_result['violation'],
            'confidence': 1.0,
            'primary_detector': 'Rule Engine'
        }
    
    # Priority 2: Statistical anomalies
    if if_result['is_anomaly']:
        return {
            'decision': 'FLAGGED',
            'reason': f"Statistical anomaly: {if_result['anomaly_score']:.4f}",
            'confidence': abs(if_result['anomaly_score']),
            'primary_detector': 'Isolation Forest'
        }
    
    # Priority 3: Behavioral anomalies
    if ae_result['is_anomaly']:
        return {
            'decision': 'FLAGGED',
            'reason': f"Behavioral anomaly: {ae_result['reconstruction_error']:.4f}",
            'confidence': ae_result['reconstruction_error'] / ae_result['threshold'],
            'primary_detector': 'Autoencoder'
        }
    
    # All clear
    return {
        'decision': 'APPROVED',
        'reason': 'All detection layers passed',
        'confidence': 0.95,
        'primary_detector': 'Combined System'
    }
```

### **Stage 5: Result Processing & Output**

#### **Decision Output Structure**
```python
{
    # Final Decision
    'final_decision': 'BLOCKED/FLAGGED/APPROVED',
    'confidence_score': 0.85,
    'risk_level': 'HIGH/MEDIUM/LOW',
    
    # Individual Layer Results
    'rule_engine': {
        'blocked': False,
        'violations': [],
        'checks_passed': ['velocity', 'amount_limits']
    },
    'isolation_forest': {
        'is_anomaly': True,
        'anomaly_score': -0.15,
        'confidence': 0.75
    },
    'autoencoder': {
        'is_anomaly': False,
        'reconstruction_error': 0.023,
        'threshold': 0.045,
        'confidence': 0.51
    },
    
    # Feature Analysis
    'key_features': {
        'transaction_amount': 5000.0,
        'deviation_from_avg': 3500.0,
        'user_avg_amount': 1500.0,
        'txn_count_1hour': 3
    },
    
    # Recommendations
    'recommended_action': 'Manual Review Required',
    'review_priority': 'HIGH',
    'additional_checks': ['Verify beneficiary', 'Contact customer']
}
```

## ğŸ“ˆ **Data Flow Performance Metrics**

### **Processing Times**
```
â±ï¸ Stage Performance
â”œâ”€â”€ Feature Engineering: ~10ms
â”œâ”€â”€ Rule Engine: ~2ms
â”œâ”€â”€ Isolation Forest: ~15ms
â”œâ”€â”€ Autoencoder: ~25ms
â”œâ”€â”€ Decision Aggregation: ~3ms
â””â”€â”€ Total Processing: ~55ms
```

### **Data Volumes**
```
ğŸ“Š Data Throughput
â”œâ”€â”€ Features per Transaction: 41
â”œâ”€â”€ Transactions per Second: 1000+
â”œâ”€â”€ Daily Transaction Volume: 100,000+
â”œâ”€â”€ Feature Data Size: ~2KB per transaction
â””â”€â”€ Model Memory Usage: ~100MB total
```

## ğŸ”„ **Data Flow Monitoring**

### **Key Monitoring Points**
```
ğŸ“Š Monitoring Dashboard
â”œâ”€â”€ ğŸ“ˆ Feature Quality Metrics
â”‚   â”œâ”€â”€ Missing feature rates
â”‚   â”œâ”€â”€ Feature distribution drift
â”‚   â”œâ”€â”€ Outlier detection rates
â”‚   â””â”€â”€ Data quality scores
â”œâ”€â”€ ğŸ¯ Model Performance
â”‚   â”œâ”€â”€ Prediction latencies
â”‚   â”œâ”€â”€ Anomaly detection rates
â”‚   â”œâ”€â”€ False positive rates
â”‚   â””â”€â”€ Model accuracy metrics
â”œâ”€â”€ ğŸš¨ System Health
â”‚   â”œâ”€â”€ Processing throughput
â”‚   â”œâ”€â”€ Error rates by stage
â”‚   â”œâ”€â”€ Memory usage patterns
â”‚   â””â”€â”€ Response time distributions
â””â”€â”€ ğŸ’¼ Business Metrics
    â”œâ”€â”€ Fraud detection rates
    â”œâ”€â”€ Customer impact scores
    â”œâ”€â”€ Cost savings metrics
    â””â”€â”€ Compliance adherence
```

### **Data Quality Checks**
```python
# Automated Data Validation
def validate_data_flow():
    # Feature completeness
    assert all(feature in transaction for feature in MODEL_FEATURES)
    
    # Value ranges
    assert 0 <= transaction_amount <= MAX_AMOUNT
    assert 0 <= hour <= 23
    assert 0 <= day_of_week <= 6
    
    # Logical consistency
    assert weekly_total >= daily_total
    assert monthly_total >= weekly_total
    assert user_max_amount >= transaction_amount
    
    # Model input validation
    assert len(feature_vector) == 41
    assert not np.any(np.isnan(feature_vector))
    assert not np.any(np.isinf(feature_vector))
```

## ğŸ¯ **Data Flow Optimization**

### **Performance Optimizations**
- **Feature Caching**: Cache user behavioral features for repeat customers
- **Batch Processing**: Process multiple transactions together when possible
- **Model Caching**: Keep models loaded in memory with @st.cache_resource
- **Parallel Processing**: Run Isolation Forest and Autoencoder in parallel

### **Scalability Considerations**
- **Horizontal Scaling**: Stateless design allows multiple instances
- **Database Optimization**: Efficient queries for historical data
- **Memory Management**: Optimized feature storage and retrieval
- **Load Balancing**: Distribute processing across multiple servers

This comprehensive data flow ensures robust, scalable, and accurate fraud detection while maintaining high performance and reliability for real-time transaction processing.